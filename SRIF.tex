\section{Square Root Information Filters}

This is a variation of Information Filters
along the same direction as
the Potter variation of the Kalman Filter.
The SRIF maintains an upper triangular matrix $I_m$
and a vector $I_v$ as the filter state.
The relationship between the filter state
and the system state (and our certainty)
is best captured by the ``Data Equation''
\begin{equation}
  I_v = I_m x + N(0,I)
\label{srifdataequation}
\end{equation}
or more informally,
``$I_v$ is $I_m$ times $x$,
plus or minus one-ish.''
Initialization of the SRIF
is best accomplished by
filling $I_m$ and $I_v$ with zeros,
then presenting whatever is known about the system
as a sequence of noisy observations.
The best estimate of the state of the system
would be whatever vector $x$
minimizes the required $N(0,I)$ noise value,
in a least-squares sense; that is, the one
that minimzes the performance functional
\begin{equation}
  J(x) = \lVert I_m x - I_v \rVert^2
\label{srifperf}
\end{equation}
If $I_m$ is invertable, then we can
rearrange (\ref{srifdataequation})
as (\ref{srifstateeqn})
giving state estimate (\ref{srifx}),
and error covaraince (\ref{srifp}):
\begin{subequations}
\begin{align}
  x & = I_m^{-1} I_v + N(0,I_m^{-1} I_m^{-t})
\label{srifstateeqn}
\\
  {\tilde x} & = I_m^{-1} I_v
\label{srifx}
\\
  {\tilde P} & = I_m^{-1}I_m^{-t}
\label{srifp}
\end{align}
\end{subequations}

See \verb|srif_read_c.c|, \verb|srif_read_f.f| and \verb|srif_read.m|
for implementations of \ref{srifx} and \ref{srifp}.

\newpage
\subsection{Refinement}

Updating the filter based on incoming information
from a noisy set of sensors
is just a larger least-squares problem.
The new best estimate for the state of the system
is again a state $x$ that minimizes,
in a least squares sense,
a system of equations;
but now the system includes not only the information
from the filter (\ref{srifstate1})
but also the information from the observation (\ref{srifobs}):
\begin{subequations}
\begin{align}
  {\tilde I}_v & = {\tilde I}_m x + N(0,I)
\label{srifstate1}
\\
  z & = H x + N(0,I)
\label{srifobs}
\end{align}
\end{subequations}
The system state, then, must minimize the performance functional
(\ref{srifdataperf1}),
which can be reorganized into a matrix equation
(\ref{srifdataperf2}):
\begin{subequations}
\begin{align}
J(x) & =  \lVert {\tilde I}_m x   - {\tilde I}_v \rVert ^ 2
       +  \lVert            H x   - z            \rVert ^ 2
\label{srifdataperf1}
\\
  & = \begin{Vmatrix}
        \begin{bmatrix} {\tilde I}_m \\ H \end{bmatrix} x -
        \begin{bmatrix} {\tilde I}_v \\ z \end{bmatrix}
      \end{Vmatrix}^2
\label{srifdataperf2}
\end{align}
\end{subequations}
Equation (\ref{srifdataperf2}) is attempting to minimize
the square of the two-norm of a vector quantity.
We can, in fact, apply any orthogonal transform we want
to the vector being minimized,
without impacting what value of $x$ will minimize it,
so we can set up the modified performance functional,
\begin{align}
J(x) & = \begin{Vmatrix}
      T \begin{bmatrix} {\tilde I}_m \\ H \end{bmatrix} x -
      T \begin{bmatrix} {\tilde I}_v \\ z \end{bmatrix}
      \end{Vmatrix}^2
\label{srifdataperf3}
\end{align}
If we pick up the data being transformed,
treat it as a matrix,
and use Householder transforms
to partially triangularize that matrix
we get an interesting result:
\begin{subequations}
\begin{align}
T \begin{bmatrix}
     {\tilde I}_m  &  {\tilde I}_v
  \\ H             &  z
  \end{bmatrix}
& =
  \begin{bmatrix}
     {\hat   I}_m  &  {\hat   I}_v
  \\ 0             &  e
  \end{bmatrix}
\label{srifdatahouse1}
\end{align}
\end{subequations}
The names selected and applied to the matrix on the right
hand side indicate where this is going. It is worthwhile
to show that, in fact, the quantities produced fit those usages.
Substituting them back into (\ref{srifdataperf2}), we get
\begin{align}
J(x) & = \begin{Vmatrix}
         T \begin{bmatrix} {\hat I}_m \\ 0 \end{bmatrix} x -
         T \begin{bmatrix} {\hat I}_v \\ e \end{bmatrix}
         \end{Vmatrix}^2
\label{srifdataperf4}
\\
  & =  \lVert {\hat I}_m x
    -  {\hat I}_v \rVert ^ 2
    +  \lVert e   \rVert ^ 2
\label{srifdataperf5}
\end{align}
The final form (\ref{srifdataperf5}) shows that
the data generated by the Householder Triangularization
can be used as the new filter state.
The final $\lVert e \rVert ^ 2$ factor represents
a minimum value for the $J(x)$ function that
no possible value of $x$ can reduce.

See \verb|srif_data_c.c|, \verb|srif_data_f.f| and \verb|srif_data.m|
for implementations of the
Square Root Information Filter
Data Processing algorithm.

\newpage
\subsection{Prediction}

Updating the Filter state so it estimates the state
of the system after a state transition
is yet another least-squares minimization problem.
We note the system state transition function (\ref{srifsys}).
The current state of the system minimizes,
in a least squares sense,
the required $N(0,I)$ noise vector in (\ref{srifstate2}).
What is known about the process noise can be placed
into a similar appropriate form (\ref{srifsysnoise}).
\begin{subequations}
\begin{align}
   y & = F x + T_w w                                 \label{srifsys}
\\ {\hat I}_v    & = {\hat I}_m   x + N(0,I)   \label{srifstate2}
\\ {\hat I}_{vw} & = {\hat I}_{mw} w + N(0,I)   \label{srifsysnoise}
\end{align}
\end{subequations}
From the above equations,
we can write a performance function $J(x,w)$
that is by definition minimized by our
estimated system state $x$ and
expected system process noise $w$:
\begin{equation}
J(x,w) =  \lVert {\hat I}_m    x  -  {\hat I}_v    \rVert ^ 2
       +  \lVert {\hat I}_{mw} w  -  {\hat I}_{vw} \rVert ^ 2
\label{sriftimeperf1}
\end{equation}
We want to estimate the next system state $y$
so we solve for $x$ in (\ref{srifsys}), then substutute
$x = F^{-1} \left( y - T_w w \right)$ into (\ref{sriftimeperf1})
to give us:
\begin{equation}
J(y,w) =  \lVert {\hat I}_m F^{-1} \left( y - T_w w \right) -
                 {\hat I}_v                       \rVert ^ 2
       +  \lVert {\hat I}_{mw} w -  {\hat I}_{vw} \rVert ^ 2
\label{sriftimeperf2}
\end{equation}
We can apply the same method as before,
writing this as a matrix equation,
applying a Householder transformation,
and unpacking the results:
\begin{subequations}
\begin{align}
J(y,w)
  & = \begin{Vmatrix}
        \begin{bmatrix}
              {\hat I}_{mw}         & 0
          \\ -{\hat I}_m F^{-1} T_w & {\hat I}_m F^{-1}
        \end{bmatrix}
        \begin{bmatrix} w \\ y \end{bmatrix} -
        \begin{bmatrix} {\hat I}_{vw} \\ {\hat I}_v \end{bmatrix}
      \end{Vmatrix}^2
\label{sriftimeperf3}
\\
  & = \begin{Vmatrix}
      T \begin{bmatrix}
              {\hat I}_{mw}         & 0
          \\ -{\hat I}_m F^{-1} T_w & {\hat I}_m F^{-1}
        \end{bmatrix}
        \begin{bmatrix} w \\ y \end{bmatrix} -
      T \begin{bmatrix} {\hat I}_{vw} \\ {\hat I}_v \end{bmatrix}
      \end{Vmatrix}^2
\\
  & = \begin{Vmatrix}
        \begin{bmatrix}
           T_1 & T_2
        \\ 0                  & {\tilde I}_m
        \end{bmatrix}
        \begin{bmatrix} w \\ y \end{bmatrix} -
        \begin{bmatrix} T_3 \\ {\tilde I}_v \end{bmatrix}
      \end{Vmatrix}^2
\label{sriftimeperf4}
\\
J(y,w)
  & = \lVert T_1 w + T_2 y - T_3 \rVert^2
\label{sriftimeperf5}
\\
  & + \lVert {\tilde I}_m y -
             {\tilde I}_v \rVert^2
\label{sriftimeperf6}
\end{align}
\end{subequations}
Bierman proves that (\ref {sriftimeperf5}) can be minimized
to zero for any ${y}$ so that the actual state estimate
is given by (\ref{sriftimeperf6}),
giving us the new data for the filter state.

See \verb|srif_time_c.c|, \verb|srif_time_f.f| and \verb|srif_time.m|
for implementations.
